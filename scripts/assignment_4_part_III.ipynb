{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "m3_assignment_part_III.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anoted/Assignment-4/blob/main/scripts/assignment_4_part_III.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part III\n",
        "Using the previous two tutorials, please answer the following using an encorder-decoder approach and an LSTM compared approach.\n",
        "\n",
        "Please create a transformer-based classifier for English name classification into male or female.\n",
        "\n",
        "There are several datasets for name for male or female classification. In subseuqent iterations, this could be expanded to included more classifications.\n",
        "\n",
        "Below is the source from NLTK, which only has male and female available but could be used for the purposes of this assignment.\n",
        "\n",
        "```\n",
        "names = nltk.corpus.names\n",
        "names.fileids()\n",
        "['female.txt', 'male.txt']\n",
        "male_names = names.words('male.txt')\n",
        "female_names = names.words('female.txt')\n",
        "[w for w in male_names if w in female_names]\n",
        "['Abbey', 'Abbie', 'Abby', 'Addie', 'Adrian', 'Adrien', 'Ajay', 'Alex', 'Alexis',\n",
        "'Alfie', 'Ali', 'Alix', 'Allie', 'Allyn', 'Andie', 'Andrea', 'Andy', 'Angel',\n",
        "'Angie', 'Ariel', 'Ashley', 'Aubrey', 'Augustine', 'Austin', 'Averil', ...]\n",
        "```"
      ],
      "metadata": {
        "id": "QD5ia2HsZpsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-nightly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9z1DQanXBTm",
        "outputId": "cca960c8-d9c3-4fe2-be3d-7e2b9c5f8d53"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-nightly in /usr/local/lib/python3.10/dist-packages (3.3.0.dev2024042203)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras-nightly) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-nightly) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-nightly) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras-nightly) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-nightly) (3.9.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras-nightly) (0.11.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras-nightly) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras-nightly) (4.11.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-nightly) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-nightly) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-nightly) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "\n",
        "import tensorflow as tf\n",
        "import string\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras import ops\n",
        "from keras import layers\n",
        "import random\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('names')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCO-HQCZ0PUI",
        "outputId": "18bbc898-e38d-4ef8-dc1d-8d667d150b3c"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "qZu8Jj0b0Ckl"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n"
      ],
      "metadata": {
        "id": "_v8sP8ivWtnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# name settings\n",
        "vocab_size = 27                     # letters + padding\n",
        "maxlen = 9                          # for name\n",
        "letters = string.ascii_lowercase    # vocabulary\n",
        "names = nltk.corpus.names           # name database\n",
        "#names.fileids()\n",
        "\n",
        "male_names = names.words('male.txt')\n",
        "female_names = names.words('female.txt')\n",
        "\n",
        "random.shuffle(male_names)          # shuffling for better training\n",
        "random.shuffle(female_names)\n",
        "\n",
        "##### position and listing\n",
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "for name in male_names[:len(male_names)-200]:\n",
        "    name = re.sub(r'[^A-Za-z]', '', name) # letter only\n",
        "    name_letters = list(name.lower())     # lower and list\n",
        "    x_name = []\n",
        "    for letter in name_letters:\n",
        "        #print(letter)\n",
        "        x_name.append(letters.index(letter)+1) # index as alphabet index 1-26\n",
        "    x_train.append(x_name)\n",
        "    y_train.append(1)                     # 1 -> male\n",
        "\n",
        "for name in female_names[:len(female_names)-200]:\n",
        "    name = re.sub(r'[^A-Za-z]', '', name) # letter only\n",
        "    name_letters = list(name.lower())     # lower and list\n",
        "    x_name = []\n",
        "    for letter in name_letters:\n",
        "        #print(letter)\n",
        "        x_name.append(letters.index(letter)+1) # index as alphabet index 1-26\n",
        "    x_train.append(x_name)\n",
        "    y_train.append(0)                     # 1 -> female\n",
        "\n",
        "# validation set\n",
        "x_val = []\n",
        "y_val = []\n",
        "\n",
        "for name in male_names[len(male_names)-200:]:\n",
        "    name = re.sub(r'[^A-Za-z]', '', name) # letter only\n",
        "    name_letters = list(name.lower())     # lower and list\n",
        "    x_name = []\n",
        "    for letter in name_letters:\n",
        "        #print(letter)\n",
        "        x_name.append(letters.index(letter)+1) # index as alphabet index 1-26\n",
        "    x_val.append(x_name)\n",
        "    y_val.append(1)                     # 1 -> male\n",
        "\n",
        "for name in female_names[len(female_names)-200:]:\n",
        "    name = re.sub(r'[^A-Za-z]', '', name) # letter only\n",
        "    name_letters = list(name.lower())     # lower and list\n",
        "    x_name = []\n",
        "    for letter in name_letters:\n",
        "        #print(letter)\n",
        "        x_name.append(letters.index(letter)+1) # index as alphabet index 1-26\n",
        "    x_val.append(x_name)\n",
        "    y_val.append(0)                     # 1 -> female\n",
        "\n",
        "# print(x_train)\n",
        "# print(x_val)\n",
        "x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val = keras.utils.pad_sequences(x_val, maxlen=maxlen)\n",
        "\n",
        "# numpy array or tensorflow tensor ...\n",
        "y_train = np.array(y_train)\n",
        "y_val = np.array(y_val)\n",
        "#x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
        "#x_val = tf.convert_to_tensor(x_val, dtype=tf.float32)\n"
      ],
      "metadata": {
        "id": "nNtTmPr3W3Ms"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model definition\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim) # attention layer\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]      # feed forward - dense\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)                    # normalizations\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)   # token embedding - self trained\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)         # positional embedding - self calculated\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = ops.shape(x)[-1]\n",
        "        positions = ops.arange(start=0, stop=maxlen, step=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n"
      ],
      "metadata": {
        "id": "Ux-EG98FimDw"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim =  64  # Embedding size for each token\n",
        "num_heads = 3   # Number of attention heads\n",
        "ff_dim = 64     # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "# model definition\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"leaky_relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "W480Y3mEW3tH"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit( x_train, y_train,\n",
        "                     batch_size=32, epochs=15,\n",
        "                     validation_data=(x_val, y_val) )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSvvatvkXjBq",
        "outputId": "a6a5bce4-701d-42c0-946e-2735f11fc60f"
      },
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.6657 - loss: 0.5998 - val_accuracy: 0.7650 - val_loss: 0.4920\n",
            "Epoch 2/15\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.7895 - loss: 0.4508 - val_accuracy: 0.7675 - val_loss: 0.5063\n",
            "Epoch 3/15\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 33ms/step - accuracy: 0.7812 - loss: 0.4393 - val_accuracy: 0.7500 - val_loss: 0.5405\n",
            "Epoch 4/15\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.7981 - loss: 0.4238 - val_accuracy: 0.7750 - val_loss: 0.5061\n",
            "Epoch 5/15\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.7987 - loss: 0.4171 - val_accuracy: 0.7725 - val_loss: 0.4970\n",
            "Epoch 6/15\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.7983 - loss: 0.4131 - val_accuracy: 0.7625 - val_loss: 0.5029\n",
            "Epoch 7/15\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.8082 - loss: 0.4136 - val_accuracy: 0.7900 - val_loss: 0.4622\n",
            "Epoch 8/15\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.8116 - loss: 0.4013 - val_accuracy: 0.7900 - val_loss: 0.4537\n",
            "Epoch 9/15\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.8126 - loss: 0.3894 - val_accuracy: 0.7750 - val_loss: 0.5114\n",
            "Epoch 10/15\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.8136 - loss: 0.3903 - val_accuracy: 0.7775 - val_loss: 0.5017\n",
            "Epoch 11/15\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.8142 - loss: 0.3805 - val_accuracy: 0.7700 - val_loss: 0.4808\n",
            "Epoch 12/15\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.8177 - loss: 0.3852 - val_accuracy: 0.7925 - val_loss: 0.4841\n",
            "Epoch 13/15\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8183 - loss: 0.3778 - val_accuracy: 0.7725 - val_loss: 0.4917\n",
            "Epoch 14/15\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8207 - loss: 0.3814 - val_accuracy: 0.7775 - val_loss: 0.4876\n",
            "Epoch 15/15\n",
            "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.8326 - loss: 0.3629 - val_accuracy: 0.7875 - val_loss: 0.4903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_name(name, maxlen=maxlen):\n",
        "    name = re.sub(r'[^A-Za-z]', '', name) # letter only\n",
        "    name_letters = list(name.lower())     # lower and list\n",
        "    x_name = []\n",
        "    for letter in name_letters:\n",
        "        #print(letter)\n",
        "        x_name.append(letters.index(letter)+1) # index as alphabet index 1-26\n",
        "    return keras.utils.pad_sequences([x_name], maxlen=maxlen)"
      ],
      "metadata": {
        "id": "kyfPiVhIkUVe"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_names = [\"Robin\", \"Sam\", \"Abigail\", \"Elliot\", \"Haley\", \"Harvey\", \"Leah\", \"Penny\"]"
      ],
      "metadata": {
        "id": "jkv1QN_Qlswd"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name in test_names:\n",
        "    predicted = model.predict(prep_name(name), verbose=0)\n",
        "    print(name, \"    \\t\",   \"male\" if np.argmax(predicted) == 1 else \"female\" )\n",
        "    print(\"------------------------\")\n",
        "\n",
        "\n",
        "### END CODE HERE ###"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_ocTVWzYlIy",
        "outputId": "1077c40c-3b68-4705-8953-7e280be53132"
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Robin     \t female\n",
            "------------------------\n",
            "Sam     \t male\n",
            "------------------------\n",
            "Abigail     \t female\n",
            "------------------------\n",
            "Elliot     \t male\n",
            "------------------------\n",
            "Haley     \t female\n",
            "------------------------\n",
            "Harvey     \t male\n",
            "------------------------\n",
            "Leah     \t female\n",
            "------------------------\n",
            "Penny     \t female\n",
            "------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "1. https://arxiv.org/pdf/2102.03692.pdf\n",
        "2. https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/exercise/13-attention.html\n",
        "3. https://towardsdatascience.com/deep-learning-gender-from-name-lstm-recurrent-neural-networks-448d64553044\n",
        "4. https://www.nltk.org/book/ch02.html#sec-lexical-resources"
      ],
      "metadata": {
        "id": "ExMITGgCdQWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PHiDsdXLhbbW"
      }
    }
  ]
}